# -*- coding: utf-8 -*-
"""stress_predictor_colab_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3AgIdjuqbtdTQzZaCx38D6JXVRUlp15

# Student Lifestyle → Stress Predictor (Colab-ready)
**What this notebook does:**

- Load the student lifestyle CSV (you can upload directly in Colab or mount Google Drive)
- Perform EDA and visualize relationships
- Preprocess features and encode target (Low / Moderate / High)
- Train baseline models (Logistic Regression, RandomForest)
- Evaluate with classification report, confusion matrix, and feature importance
- Save trained model (`joblib`) and provide a simple Streamlit demo script

**How to use in Google Colab:**
1. Upload your CSV using the file upload cell below, or mount Google Drive and place the CSV in your Drive.
2. Run cells in order.

> Notebook saved as: `/content/stress_predictor_colab_notebook.ipynb` (also provided for download)
"""

# Install extra libs if needed (uncomment if required in Colab)
# !pip install xgboost lightgbm streamlit --quiet

import pandas as pd, numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
import joblib
import os
print('Libraries loaded. Current working dir:', os.getcwd())

# === DATA LOADING ===
# Option 1: Upload directly in Colab (uncomment to use)
from google.colab import files
uploaded = files.upload()  # run this cell and select your CSV file
# After upload, pick the first csv file in uploaded
csv_path = list(uploaded.keys())[0]
print('Uploaded file:', csv_path)

# Option 2: If using Google Drive, uncomment and run:
# from google.colab import drive
# drive.mount('/content/drive')
# csv_path = '/content/drive/MyDrive/your_folder/student_lifestyle.csv'

# Option 3: If your file is already in the runtime (e.g., uploaded by other means), set path manually:
# csv_path = '/content/student_lifestyle.csv'

df = pd.read_csv(csv_path)
print('Loaded dataset with shape:', df.shape)
df.head()

# === QUICK EDA ===
print('Columns:', df.columns.tolist())
print('\nTarget distribution:')
print(df['Stress_Level'].value_counts())

# Basic stats
display(df.describe())

# Pairwise relationships (basic)
plt.figure(figsize=(10,6))
df[['Study_Hours_Per_Day','Sleep_Hours_Per_Day','Social_Hours_Per_Day','Physical_Activity_Hours_Per_Day','GPA']].hist(bins=12, figsize=(12,8))
plt.tight_layout()
plt.show()

# Correlation heatmap (matplotlib only)
corr = df.select_dtypes(include=[np.number]).corr()
plt.figure(figsize=(8,6))
plt.imshow(corr, interpolation='none', aspect='auto')
plt.colorbar()
plt.xticks(range(len(corr)), corr.columns, rotation=45)
plt.yticks(range(len(corr)), corr.columns)
plt.title('Correlation matrix (numeric features)')
plt.tight_layout()
plt.show()

# Boxplots of features by Stress_Level
features = ['Study_Hours_Per_Day','Sleep_Hours_Per_Day','Social_Hours_Per_Day','Physical_Activity_Hours_Per_Day','GPA','Extracurricular_Hours_Per_Day']
for f in features:
    plt.figure(figsize=(6,3))
    df.boxplot(column=f, by='Stress_Level')
    plt.title(f + ' by Stress_Level')
    plt.suptitle('')
    plt.show()

# === PREPROCESSING ===
data = df.copy()

# Encode target
le = LabelEncoder()
data['stress_label'] = le.fit_transform(data['Stress_Level'])  # Low=0, Moderate=1, High=2 (check mapping)
print('Label mapping:', dict(zip(le.classes_, le.transform(le.classes_))))

X = data.drop(columns=['Student_ID','Stress_Level','stress_label'], errors='ignore')
y = data['stress_label']

# Train-test split (stratify by y to keep class balance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)

# Scaling numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# === MODEL TRAINING: Baselines ===
models = {
    'LogisticRegression': LogisticRegression(max_iter=1000),
    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)
    print('\n===', name, '===')
    print('Accuracy:', accuracy_score(y_test, preds))
    print('F1 (macro):', f1_score(y_test, preds, average='macro'))
    print(classification_report(y_test, preds, target_names=le.classes_))

# Confusion matrix for the best model (choose RandomForest here)
best = models['RandomForest']
y_pred = best.predict(X_test_scaled)

cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot(cmap='viridis')
plt.title('Confusion Matrix - RandomForest')
plt.show()

# Feature importance
importances = best.feature_importances_
feat_names = X.columns.tolist()
feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)
print('Feature importances:\n', feat_imp)
plt.figure(figsize=(6,4))
feat_imp.plot.bar()
plt.title('Feature importance (RandomForest)')
plt.tight_layout()
plt.show()

# Cross-validation (stratified)
from sklearn.model_selection import StratifiedKFold, cross_val_score
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
scores = cross_val_score(rf, scaler.fit_transform(X), y, cv=skf, scoring='f1_macro')
print('Cross-val F1_macro scores:', scores)
print('Mean F1_macro:', scores.mean())

# Train final RF on entire dataset and save
rf.fit(scaler.fit_transform(X), y)
joblib.dump({'model': rf, 'scaler': scaler, 'label_encoder': le}, 'stress_predictor_model.joblib')
print('Saved model to stress_predictor_model.joblib')

# === STREAMLIT DEMO (saved as streamlit_app.py) ===
streamlit_code = r"""
import streamlit as st
import pandas as pd, joblib
st.title('Student Lifestyle → Stress Predictor (Demo)')

# Load model (place stress_predictor_model.joblib in same folder)
m = joblib.load('stress_predictor_model.joblib')
model = m['model']
scaler = m['scaler']
le = m['label_encoder']

st.write('Enter student lifestyle values:')
study = st.number_input('Study hours per day', min_value=0.0, value=5.0)
extrac = st.number_input('Extracurricular hours per day', min_value=0.0, value=1.0)
sleep = st.number_input('Sleep hours per day', min_value=0.0, value=7.5)
social = st.number_input('Social hours per day', min_value=0.0, value=2.0)
physical = st.number_input('Physical activity hours per day', min_value=0.0, value=1.0)
gpa = st.number_input('GPA', min_value=0.0, max_value=4.0, value=3.0)

df = pd.DataFrame([[study, extrac, sleep, social, physical, gpa]], columns=['Study_Hours_Per_Day','Extracurricular_Hours_Per_Day','Sleep_Hours_Per_Day','Social_Hours_Per_Day','Physical_Activity_Hours_Per_Day','GPA'])
X = scaler.transform(df)
pred = model.predict(X)[0]
proba = model.predict_proba(X)[0].max()
st.write('Predicted stress level:', le.inverse_transform([pred])[0])
st.write('Confidence:', float(proba))
"""

with open('streamlit_app.py','w') as f:
    f.write(streamlit_code)
print('Saved simple Streamlit demo to streamlit_app.py. Run with: streamlit run streamlit_app.py')

from google.colab import files
uploaded = files.upload()   # choose streamlit_app.py and stress_predictor_model.joblib
for k in uploaded.keys():
    print("Uploaded:", k)

# Colab cell (prefix with !)
!pip install -q streamlit==1.30.0 sklearn pandas joblib
# Install Node (for npx / localtunnel)
!apt-get update -qq && apt-get install -y -qq nodejs npm
# Install localtunnel globally (npx also works without global install)
!npm install -g localtunnel

# === Single paste-and-run block for Colab ===
# 1) Install streamlit
!pip install -q streamlit==1.30.0

# 2) Download ngrok binary (no apt/npm)
!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip -o ngrok.zip >/dev/null

# 3) Make sure your app files are in the current dir: streamlit_app.py and stress_predictor_model.joblib
echo "Files in current dir:"
ls -la

# 4) Ask for your ngrok authtoken (you will paste it into the input box)
from getpass import getpass
NGROK_AUTH = getpass("Paste your ngrok authtoken (won't be shown): ")

# 5) Configure ngrok with your token
import os, subprocess, time, json, requests, sys
if NGROK_AUTH.strip()=="":
  print("No authtoken provided. If you prefer not to use ngrok, see the README for other deployment options.")
else:
  !./ngrok authtoken {NGROK_AUTH}

# 6) Start streamlit in background (writes logs to streamlit_log.txt)
# If your files are in a different folder, change the path or move them here.
print("Starting Streamlit (background)...")
get_ipython().system_raw("streamlit run streamlit_app.py --server.port=8501 &> streamlit_log.txt &")
time.sleep(3)

# 7) Start ngrok tunnel to port 8501 (background)
print("Starting ngrok tunnel...")
get_ipython().system_raw("./ngrok http 8501 --log=stdout &> ngrok_log.txt &")
time.sleep(3)

# 8) Fetch the public URL from ngrok's local API
def get_ngrok_url():
    try:
        resp = requests.get("http://127.0.0.1:4040/api/tunnels")
        tunnels = resp.json().get("tunnels", [])
        if len(tunnels)==0:
            return None
        # prefer https tunnel
        for t in tunnels:
            if t.get("public_url","").startswith("https"):
                return t["public_url"]
        return tunnels[0].get("public_url")
    except Exception as e:
        return None

public_url = None
for i in range(10):
    public_url = get_ngrok_url()
    if public_url:
        break
    time.sleep(1)
if public_url:
    print("Your Streamlit app should be reachable at:\n", public_url)
else:
    print("Couldn't fetch ngrok URL. Check ngrok_log.txt and streamlit_log.txt for errors.")

# 9) Show last 200 lines of Streamlit logs for debugging
print("\n--- Streamlit log (last 200 lines) ---")
!tail -n 200 streamlit_log.txt || true

print("\n--- ngrok log (last 200 lines) ---")
!tail -n 200 ngrok_log.txt || true

print("\nTo stop the app, run: !pkill -f streamlit")
print("To stop ngrok, run: !pkill -f ngrok")

# Single cell to run Streamlit + ngrok in Colab (uses pyngrok)
# Make sure you uploaded streamlit_app.py and stress_predictor_model.joblib to /content
# or mounted Drive and %cd into the folder where they are.

# 1) Install dependencies
!pip install -q streamlit==1.30.0 pyngrok==5.2.1

# 2) Quick check: list files so you can confirm streamlit_app.py and model are present
import os, time
print("Working dir:", os.getcwd())
print("Files:")
!ls -la

# If your files are in Drive, uncomment & edit the next lines:
# from google.colab import drive
# drive.mount('/content/drive')
# %cd /content/drive/MyDrive/your_folder_with_files

# 3) Ask for ngrok authtoken (paste it when prompted)
from getpass import getpass
NGROK_AUTH = getpass("Paste your ngrok authtoken (from https://dashboard.ngrok.com): ").strip()

# 4) Configure pyngrok / ngrok
from pyngrok import ngrok, conf
if NGROK_AUTH:
    conf.get_default().auth_token = NGROK_AUTH
    # register token (pyngrok will handle underlying ngrok binary download)
    try:
        ngrok.set_auth_token(NGROK_AUTH)
    except Exception as e:
        # not fatal — pyngrok will still try to download & run ngrok
        print("Warning while setting token:", e)

# 5) Start Streamlit in background and capture logs
print("Starting Streamlit (background)...")
get_ipython().system_raw("streamlit run streamlit_app.py --server.port=8501 &> streamlit_log.txt &")
time.sleep(2)

# 6) Open ngrok tunnel and print public URL
print("Opening ngrok tunnel...")
public_url = None
try:
    tunnel = ngrok.connect(8501, bind_tls=True)
    public_url = tunnel.public_url
    print("Public URL (open in browser):", public_url)
except Exception as e:
    print("Failed to start ngrok tunnel:", e)

# 7) Show logs (first few lines) to help debugging
print("\n--- Streamlit log (last 200 lines) ---")
!tail -n 200 streamlit_log.txt || true

print("\nIf the app doesn't load, check:")
print("- streamlit_log.txt above for Python exceptions")
print("- that 'streamlit_app.py' and 'stress_predictor_model.joblib' are in the current directory")
print("\nTo stop Streamlit: !pkill -f streamlit")
print("To stop ngrok:  ngrok.disconnect(tunnel.public_url)  (or) !pkill -f ngrok")

"""
---

## Files this notebook produces
- `stress_predictor_model.joblib`  — trained model + scaler + label encoder
- `streamlit_app.py` — a minimal demo app

## Notes & next steps
- You can try XGBoost / LightGBM for possibly better results (install in Colab).
- If classes are imbalanced, use class_weight or resampling.
- For deployment: either run Streamlit locally or convert model into an API using FastAPI.

Good luck! If you want, I can also:
- generate a ready-to-run **Google Colab link** (nb hosted) or
- create presentation slides from this notebook, or
- produce a cleaned README and a zip with code & model.
"""